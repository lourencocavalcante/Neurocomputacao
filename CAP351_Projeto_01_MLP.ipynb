{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAP351_Projeto_01_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPEiJbOyUC3whzRvA2ByHUb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lourencocavalcante/Neurocomputacao/blob/main/CAP351_Projeto_01_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[<img src=\"https://github.com/lourencocavalcante/LogosINPE/blob/main/logoinpe.png?raw=true\" width = 500 align=\"left\">](https://www.gov.br/inpe/pt-br)\n",
        "\n",
        "[<img src=\"https://github.com/lourencocavalcante/LogosINPE/blob/main/LogoCAP.png?raw=true\" width = 300 align=\"right\">](http://www.inpe.br/posgraduacao/cap/)"
      ],
      "metadata": {
        "id": "CSLQp4jYNHcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CAP-351 Neurocomputação**\n",
        "\n",
        "**Professor:**\n",
        "*Dr. Marcos Goncalves Quiles*\n",
        "\n",
        "**Aluno:**\n",
        "*Lourenço José Cavalcante Neto*\n",
        "\n",
        "---\n",
        "\n",
        "**PROPOSTA DE PROJETO 1: Experimentos com a rede Multi-layer Perceptron (MLP)**\n",
        "\n",
        "**Descrição do projeto:**\n",
        "\n",
        "1. Selecionar dois datasets (não triviais): Um dataset para classificação; Um dataset para regressão; Separar em treino/validação/teste.\n",
        "\n",
        "2. Treinar modelos MLP para os dois problemas (**classificação** e **regressão**)\n",
        "\n",
        "3. Considerar:\n",
        "  * Diferentes topologias (>=5 topologias, variar número de\n",
        "camadas)\n",
        "  * Usar o algoritmo original SGD (não usar algoritmos otimizados,\n",
        "e.g. ADAM)\n",
        "  * Avaliar o impacto do uso do Momentum\n",
        "  * Avaliar o impacto do uso da regularização (i.e. L2)\n",
        "\n",
        "4. Ilustrar graficamente a evolução do treinamento (treino/validação).\n",
        "\n",
        "5. Confeccionar um relatório (reprodutível) contendo os\n",
        "experimentos e resados"
      ],
      "metadata": {
        "id": "4Uk7l1OnKjYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introdução**\n",
        "\n",
        "A larga aplicabilidade das redes neurais se justifica pelo fato da existência de vários de modelos presentes na literatura no tocante deste tema, nas quais possuem diferentes modos, paradigmas e formas de aprendizagem. Destaca-se a rede Multilayer Perceptron, denominada por (BASHEER; HAJMEER, 2000) como o “workhorse”das redes neurais, sendo este um dos modelos mais largamente utilizado (BASHEER; HAJMEER, 2000).\n",
        "\n",
        "No presente notebook contém diversos experimentos e testes com a rede Multi-layer Perceptron (MLP). Foi selecionado um datasets (não trivial) no qual foi divido em treino, validação e teste, e utilizado para treino de modelos MLP em dois problemas: classificação e regressão. Os Dados são de um equipamento chamado **Disdrômetro RD80** (também chamado de **Joss**). Instalado próximo à torre **ATTO**, em um sítio chamado Campina, na região Amazônica, o Disdrômetro RD80 mede a distribuição das gotas de chuva que chegam à superfície.\n",
        "\n",
        "Foram criadas 5 topologias diferentes, variando o número de camadas. O algoritmo utilizado foi SGD, e foram aplicadas diferentes hyperparâmetros afim de avaliar o impacto do uso do Momentum e da Regularização (L2).\n",
        "\n",
        "Este notebook pode ser acessado no **github**: https://github.com/lourencocavalcante/Neurocomputacao.git"
      ],
      "metadata": {
        "id": "UTePmjPO0HTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bibliotecas/Pacotes**"
      ],
      "metadata": {
        "id": "FxA9_U4DVVcF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jZRmEf4Kha7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**\n",
        " \n",
        "Os Dados são de um equipamento chamado **Disdrômetro RD80** (também chamado de **Joss**). Instalado próximo à torre **ATTO**, em um sítio chamado Campina, na região amazônica, o Disdrômetro RD80 mede a distribuição das gotas de chuva que chegam à superfície. No total, o Dataset possui 06 atributos, os são listados abaixo:\n",
        "\n",
        "```\n",
        "Datetime\n",
        "Rain Intensity (mm/h)\n",
        "radar reflectivity (1/mm6m3)\n",
        "Liquid watercontent (g/m3)\n",
        "Mean weight diameter(mm)\n",
        "Time integration (s)\n",
        "```\n",
        "\n",
        "*Ob.: Conforme sugerido/autorizado pelo professor Marcos na aula do dia 11/07, o mesmo Dataset será utilizado para os problemas de Classificação e Regressão.*"
      ],
      "metadata": {
        "id": "EyIldOmMMLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importação, Formatação, Tratamento e Visualização dos dados**"
      ],
      "metadata": {
        "id": "mKuzj0eJ8bu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando o Dataset e criando o Dataframe\n",
        "df_rd80_full = pd.read_csv(\"/content/drive/MyDrive/MestradoCAP/Lourenco/cap351/Datasets/df_final_rd80_joss.csv\")\n",
        "df_rd80_full = pd.DataFrame(df_rd80_full)\n",
        "\n",
        "#Aqui verificamos o tamanho e quantidade de atributos do Dataset\n",
        "print('Tamanho do dataset: ',df_rd80_full.shape[0], ', quantidade de atributos: ', df_rd80_full.shape[1])"
      ],
      "metadata": {
        "id": "Qvv_eiv2Hy7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, o tamanho do nosso Dataset é de 315.420 e isso pode ocasionar um custo computacional muito alto durante os experimentos e testes do Projeto. Sendo assim, será utilizado apenas 1/3 dos dados."
      ],
      "metadata": {
        "id": "qre8VpmUvboU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definindo uma parte do Dataset para usá-lo neste Projeto\n",
        "limit_date = datetime.datetime(year=2022, month=1,day=1)\n",
        "\n",
        "df_rd80_full = df_rd80_full.set_index('Datetime')\n",
        "\n",
        "df_rd80 = df_rd80_full.loc[df_rd80_full.index >= str(limit_date)]\n",
        "df_rd80 = df_rd80.loc[df_rd80['Rain Intensity (mm/h)'] > 0]\n",
        "df_rd80.reset_index(inplace=True)\n",
        "\n",
        "print('Tamanho do Dataset que será utilizado: ', df_rd80.shape[0])\n"
      ],
      "metadata": {
        "id": "b8QBsVdza6Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carregando o Dataset\n",
        "df_rd80.loc[df_rd80['Rain Intensity (mm/h)'] > 0].head()"
      ],
      "metadata": {
        "id": "lNxc5-Vva_tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalizando os dados\n",
        "#É necessário assumir que os valores para Rain Intensity (mm/h) que forem menores que 0.1 sejam considerados como 0(zero)\n",
        "df_rd80['Rain Intensity (mm/h)'].loc[df_rd80['Rain Intensity (mm/h)'] < 0.1] = 0"
      ],
      "metadata": {
        "id": "Jsqf8diZMPnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizando as correlações entre as variáveis\n",
        "corr = df_rd80.corr()\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(corr[(corr >= 0.30) | (corr <= -0.30)],cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1, annot=True, annot_kws={\"size\": 14}, square=True);"
      ],
      "metadata": {
        "id": "v9pz9gxXfHl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(12,5),dpi=80)\n",
        "plt.scatter(df_rd80['Rain Intensity (mm/h)'] ,df_rd80['Liquid watercontent (g/m3)'],s=2, color='#4682B4')\n",
        "plt.xlabel(r'Teor de água líquida (g/m3)',size=14)\n",
        "plt.ylabel(r'Intensidade da chuva (mm/h)',size=14)\n",
        "plt.title(\"Distribuição das gotas de chuva que chegam à superfície\", size=16)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "saEqKzsLffw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conforme orientações para o projeto, para realizar o treinamento e validação do aprendizado de uma rede **Multi-layer Perceptron (MLP)**, realizou-se a divisão do conjunto de dados em dois grupos: Treino (70%) e Teste (30%). A biblioteca scikit-learn nos auxiliará nesta tarefa. "
      ],
      "metadata": {
        "id": "JKofm2JV_fhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividindo os datasets de treino e teste\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_rd80['Rain Intensity (mm/h)'] ,df_rd80['Liquid watercontent (g/m3)'], test_size=0.76)\n",
        "print('Tamanho do dataset para treinamento: ',len(x_train))\n",
        "print('Tamanho do dataset para validação: ',len(x_test))"
      ],
      "metadata": {
        "id": "bG5YkThZy0iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após dividir o Dataset, inicialmente vamos checar o comportamento do dataset de Treino e de Teste da rede, para entender o que a rede buscará aprender e o que esta rede buscará prever."
      ],
      "metadata": {
        "id": "z892lYjhI80Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(9,4),dpi=80)\n",
        "plt.plot(x_train, y_train, '.', color='#FF8C00', label=\"Dados para Treinamento\")\n",
        "plt.plot(x_test, y_test, '.', color='#4682B4', label=\"Dados para Teste\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gu3oVIfJJeZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PROBLEMA 1: Regressão**\n",
        "Com o objetivo de construir uma rede capaz de encontrar uma lei de regressão para o problema, serão criadas 05(cinco) topologias de rede MLP diferentes. O primeiro caso será um simples e clássico de um perceptron e a partir do segundo será modificado o número de camadas e o número de neurônios."
      ],
      "metadata": {
        "id": "QNWZvm9A3weo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Valores para configurações dos Hiperparâmetros do Momentum e Regularização (L2) para avaliar seus impactos**"
      ],
      "metadata": {
        "id": "h8_t5IjCa2c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configurações para avaliar o impacto do uso do Momentum e da Regularização (L2)\n",
        "momentum =    [0.1, 0.2, 0.3, 0.0]\n",
        "regularizer = [0.3, 0.2, 0.1, 0.0]\n",
        "dropout = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "#Define a quantidade de épocas para o treinamento\n",
        "epochs = 1000 "
      ],
      "metadata": {
        "id": "bTNyRN1XaygL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topologia I**\n",
        "Este primeiro será um caso simples e clássico de um perceptron. A rede neural que será criada  é uma MLP com 2 camadas, sendo estas a camada de entrada e a camada de saída. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v4ViwiUIsO30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criação dos modelos**"
      ],
      "metadata": {
        "id": "Zvgv9bHmshmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelos da Topologia I\n",
        "modelos_tpl_I = []\n",
        "\n",
        "for res in range(0,4):\n",
        "  #print(i)\n",
        "  sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=momentum[res])\n",
        "  modelos_tpl_I.append(keras.Sequential())\n",
        "  modelos_tpl_I[res].add(keras.layers.Dense(units=3, kernel_regularizer=l2(regularizer[res]), input_shape=[1], activation=\"tanh\"))\n",
        "  modelos_tpl_I[res].add(keras.layers.Dense(units=1, activation=\"linear\"))\n",
        "  #tf.keras.utils.plot_model(modelos_tpl_I[res], show_shapes=True)\n",
        "  modelos_tpl_I[res].compile(loss='mean_squared_error', optimizer=sgd, metrics=[\"mean_absolute_error\"])"
      ],
      "metadata": {
        "id": "LBi2FXybgOac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agora vamos treinar os Modelos criados**"
      ],
      "metadata": {
        "id": "wcAZYVbYggPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
        "\n",
        "history_modelos_tpl_I = []\n",
        "for res in range(0,4):\n",
        "  history_modelos_tpl_I.append(modelos_tpl_I[res].fit(x_train, y_train, epochs=epochs, batch_size=10, validation_split=0.2, verbose=True)) #callbacks=[early]\n"
      ],
      "metadata": {
        "id": "OoGtBlXvgfzM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "36785050-6000-437c-dcff-9a8e11defb6d"
      },
      "execution_count": 368,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "1796/1796 [==============================] - 5s 2ms/step - loss: 0.0659 - mean_absolute_error: 0.0956 - val_loss: 0.0460 - val_mean_absolute_error: 0.1042\n",
            "Epoch 2/250\n",
            "1796/1796 [==============================] - 4s 2ms/step - loss: 0.0475 - mean_absolute_error: 0.0975 - val_loss: 0.0478 - val_mean_absolute_error: 0.0907\n",
            "Epoch 3/250\n",
            "1796/1796 [==============================] - 4s 2ms/step - loss: 0.0475 - mean_absolute_error: 0.0972 - val_loss: 0.0450 - val_mean_absolute_error: 0.0871\n",
            "Epoch 4/250\n",
            "1153/1796 [==================>...........] - ETA: 1s - loss: 0.0484 - mean_absolute_error: 0.0979"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-368-02a75f5b418b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhistory_modelos_tpl_I\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mhistory_modelos_tpl_I\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelos_tpl_I\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#callbacks=[early]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[1;32m   1378\u001b[0m                 \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1244\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m       \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m       can_run_full_execution = (\n\u001b[1;32m   1248\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    676\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   4063\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4064\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 4065\u001b[0;31m         _ctx, \"Identity\", name, input)\n\u001b[0m\u001b[1;32m   4066\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4067\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados do Treinamento**"
      ],
      "metadata": {
        "id": "CovhFhCGpqXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA I: EVOLUÇÃO DOS TREINAMENTOS', size=14)\n",
        "for res in range(0,4):\n",
        "    plt.subplot(2,2,res+1)\n",
        "    plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(history_modelos_tpl_I[res].history['loss'], label=\"MLP - Treino\")\n",
        "    plt.plot(history_modelos_tpl_I[res].history['val_loss'], label=\"MLP - Validação\")\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TwzvUCVZgpik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Avaliação**"
      ],
      "metadata": {
        "id": "vidmUKLLnlHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA I: AVALIAÇÃO', size=14)\n",
        "\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = modelos_tpl_I[res].predict(x_train)\n",
        "  plt.plot(x_train, y_train, '.', color='#4682B4')\n",
        "  plt.plot(x_train, y_rest, '.',color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EEDqJlOKnjiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA I: AVALIAÇÃO', size=14)\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = modelos_tpl_I[res].predict(x_test)\n",
        "  plt.plot(x_test, y_test, '.', color='#4682B4')\n",
        "  plt.plot(x_test, y_rest, '.', markersize=3 ,color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for res in range(0,4):\n",
        "  test_loss = modelos_tpl_I[res].evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "iz_dBr1-n2VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_modelos_tpl_I = []\n",
        "for res in range(0,4): \n",
        "  score_modelos_tpl_I.append(modelos_tpl_I[res].evaluate(x_test, y_test, verbose=0))\n",
        "  print('Topologia I - Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  print('Teste loss:',   score_modelos_tpl_I[res][0])\n",
        "  print('Teste métricas:', score_modelos_tpl_I[res][1])\n",
        "  print('----------------------------------------')"
      ],
      "metadata": {
        "id": "J3U7bmmJRxuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "obTWRjl18Nr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topologia II**\n",
        "\n",
        "A rede neural que será criada será uma MLP com 5 camadas, sendo:\n",
        "*   01 camada de entrada (input layer) com 10 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 15 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 10 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 5 Neurônios\n",
        "*   01 camada de saída (output layer) com 1 Neurônio"
      ],
      "metadata": {
        "id": "Rw0K8Fm63ymW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criação do Modelos**"
      ],
      "metadata": {
        "id": "Ib7GDXsOn__R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelos da Topologia II\n",
        "mdl_tpl_II = []\n",
        "\n",
        "for res in range(0,4):\n",
        "  #print(i)\n",
        "  sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=momentum[res])\n",
        "  mdl_tpl_II.append(keras.Sequential())\n",
        "  mdl_tpl_II[res].add(keras.layers.Dense(units=10,  kernel_regularizer=l2(regularizer[res]), input_shape=[1], activation=\"tanh\"))\n",
        "  mdl_tpl_II[res].add(keras.layers.Dense(units=15,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_II[res].add(keras.layers.Dense(units=10,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_II[res].add(keras.layers.Dense(units=5,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_II[res].add(keras.layers.Dense(units=1,  activation=\"linear\"))\n",
        "  #tf.keras.utils.plot_model(mdl_tpl_II[res], show_shapes=True)\n",
        "  mdl_tpl_II[res].compile(loss='mean_squared_error', optimizer=sgd, metrics=[\"mean_absolute_error\"])"
      ],
      "metadata": {
        "id": "C58Lj89kPMsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agora vamos treinar os Modelos criados**"
      ],
      "metadata": {
        "id": "ochZeag8Ye62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
        "\n",
        "history_mdl_tpl_II = []\n",
        "for res in range(0,4):\n",
        "  history_mdl_tpl_II.append(mdl_tpl_II[res].fit(x_train, y_train, epochs=epochs, batch_size=10, validation_split=0.2, verbose=True)) #callbacks=[early]\n"
      ],
      "metadata": {
        "id": "7pYDzqG_Ye63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados do Treinamento**"
      ],
      "metadata": {
        "id": "BQO389mxZmeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA II: EVOLUÇÃO DOS TREINAMENTOS', size=14)\n",
        "for res in range(0,4):\n",
        "    plt.subplot(2,2,res+1)\n",
        "    plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(history_mdl_tpl_II[res].history['loss'], label=\"MLP - Treino\")\n",
        "    plt.plot(history_mdl_tpl_II[res].history['val_loss'], label=\"MLP - Validação\")\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cGk7jJVNZmer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Avaliação**"
      ],
      "metadata": {
        "id": "tgcYD1YNZmer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA II: AVALIAÇÃO', size=14)\n",
        "\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = mdl_tpl_II[res].predict(x_train)\n",
        "  plt.plot(x_train, y_train, '.', color='#4682B4')\n",
        "  plt.plot(x_train, y_rest, '.',color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1_gfRRGAZmer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA II: AVALIAÇÃO', size=14)\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = mdl_tpl_II[res].predict(x_test)\n",
        "  plt.plot(x_test, y_test, '.', color='#4682B4')\n",
        "  plt.plot(x_test, y_rest, '.', markersize=3 ,color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for res in range(0,4):\n",
        "  test_loss = mdl_tpl_II[res].evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "eELp7CHNZmer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_mdl_tpl_II = []\n",
        "for res in range(0,4): \n",
        "  score_mdl_tpl_II.append(mdl_tpl_II[res].evaluate(x_test, y_test, verbose=0))\n",
        "  print('Topologia II - Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  print('Teste loss:',   score_mdl_tpl_II[res][0])\n",
        "  print('Teste métricas:', score_mdl_tpl_II[res][1])\n",
        "  print('----------------------------------------')"
      ],
      "metadata": {
        "id": "TrRIpFB_Zmes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "i28GNFRkrlCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topologia III**\n",
        "\n",
        "A rede neural que será criada será uma MLP com 5 camadas, sendo:\n",
        "*   01 camada de entrada (input layer) com 15 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 15 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 10 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 5 Neurônios\n",
        "*   01 camada de saída (output layer) com 1 Neurônio"
      ],
      "metadata": {
        "id": "heRoh2V_X-Rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criação do Modelos**"
      ],
      "metadata": {
        "id": "kH_7uFCu6_eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelos da Topologia III\n",
        "mdl_tpl_III = []\n",
        "\n",
        "for res in range(0,4):\n",
        "  #print(i)\n",
        "  sgd = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=momentum[res])\n",
        "  mdl_tpl_III.append(keras.Sequential())\n",
        "  mdl_tpl_III[res].add(keras.layers.Dense(units=15,  kernel_regularizer=l2(regularizer[res]), input_shape=[1], activation=\"tanh\"))\n",
        "  mdl_tpl_III[res].add(keras.layers.Dense(units=15,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_III[res].add(keras.layers.Dense(units=10,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_III[res].add(keras.layers.Dense(units=5,  activation=\"tanh\"))\n",
        "  mdl_tpl_III[res].add(keras.layers.Dense(units=1,  activation=\"linear\"))\n",
        "  #tf.keras.utils.plot_model(mdl_tpl_III[res], show_shapes=True)\n",
        "  mdl_tpl_III[res].compile(loss='mean_squared_error', optimizer=sgd, metrics=[\"mean_absolute_error\"])"
      ],
      "metadata": {
        "id": "bBBBbLld6dig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agora vamos treinar os Modelos criados**"
      ],
      "metadata": {
        "id": "qzEcOmHH6dig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
        "\n",
        "history_mdl_tpl_III = []\n",
        "for res in range(0,4):\n",
        "  history_mdl_tpl_III.append(mdl_tpl_III[res].fit(x_train, y_train, epochs=epochs, batch_size=10, validation_split=0.2, verbose=True)) #callbacks=[early]\n"
      ],
      "metadata": {
        "id": "mkHY9gx06dig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados do Treinamento**"
      ],
      "metadata": {
        "id": "HtoNoXoR6dig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA III: EVOLUÇÃO DOS TREINAMENTOS', size=14)\n",
        "for res in range(0,4):\n",
        "    plt.subplot(2,2,res+1)\n",
        "    plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(history_mdl_tpl_III[res].history['loss'], label=\"MLP - Treino\")\n",
        "    plt.plot(history_mdl_tpl_III[res].history['val_loss'], label=\"MLP - Validação\")\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FvtvMw7t6dih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Avaliação**"
      ],
      "metadata": {
        "id": "fZuDPCPu6dih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA III: AVALIAÇÃO', size=14)\n",
        "\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = mdl_tpl_III[res].predict(x_train)\n",
        "  plt.plot(x_train, y_train, '.', color='#4682B4')\n",
        "  plt.plot(x_train, y_rest, '.',color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h1DOxEJz6dih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA III: AVALIAÇÃO', size=14)\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = mdl_tpl_III[res].predict(x_test)\n",
        "  plt.plot(x_test, y_test, '.', color='#4682B4')\n",
        "  plt.plot(x_test, y_rest, '.', markersize=3 ,color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for res in range(0,4):\n",
        "  test_loss = mdl_tpl_III[res].evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "25May2fj6dih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_mdl_tpl_III = []\n",
        "for res in range(0,4): \n",
        "  score_mdl_tpl_III.append(mdl_tpl_III[res].evaluate(x_test, y_test, verbose=0))\n",
        "  print('Topologia III - Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  print('Teste loss:',   score_mdl_tpl_III[res][0])\n",
        "  print('Teste métricas:', score_mdl_tpl_III[res][1])\n",
        "  print('----------------------------------------')"
      ],
      "metadata": {
        "id": "jPAmRJTF6dii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dMQlGi_mGnC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topologia IV**\n",
        "\n",
        "A rede neural que será criada será uma MLP com 6 camadas, sendo:\n",
        "*   01 camada de entrada (input layer) com 20 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 20 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 20 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 10 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 5 Neurônios\n",
        "*   01 camada de saída (output layer) com 1 Neurônio"
      ],
      "metadata": {
        "id": "HaGfg7ZeE-VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criação do Modelos**"
      ],
      "metadata": {
        "id": "sJ-tTv8RE-VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelos da Topologia IV\n",
        "mdl_tpl_IV = []\n",
        "\n",
        "for res in range(0,4):\n",
        "  #print(i)\n",
        "  sgd = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=momentum[res])\n",
        "  mdl_tpl_IV.append(keras.Sequential())\n",
        "  mdl_tpl_IV[res].add(keras.layers.Dense(units=20,  kernel_regularizer=l2(regularizer[res]), input_shape=[1], activation=\"tanh\"))\n",
        "  mdl_tpl_IV[res].add(keras.layers.Dense(units=20,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_IV[res].add(keras.layers.Dense(units=20,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_IV[res].add(keras.layers.Dense(units=10,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_IV[res].add(keras.layers.Dense(units=5,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_IV[res].add(keras.layers.Dense(units=1, activation=\"linear\"))\n",
        "  #tf.keras.utils.plot_model(mdl_tpl_IV[res], show_shapes=True)\n",
        "  mdl_tpl_IV[res].compile(loss='mean_squared_error', optimizer=sgd, metrics=[\"mean_absolute_error\"])"
      ],
      "metadata": {
        "id": "aAG1xttsE-VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agora vamos treinar os Modelos criados**"
      ],
      "metadata": {
        "id": "059r8QMtE-VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
        "\n",
        "history_mdl_tpl_IV = []\n",
        "for res in range(0,4):\n",
        "  history_mdl_tpl_IV.append(mdl_tpl_IV[res].fit(x_train, y_train, epochs=epochs, batch_size=10, validation_split=0.2, verbose=True)) #callbacks=[early]\n"
      ],
      "metadata": {
        "id": "ANqy9TyfE-VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados do Treinamento**"
      ],
      "metadata": {
        "id": "Mhp6nVzhE-VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA IV: EVOLUÇÃO DOS TREINAMENTOS', size=14)\n",
        "for res in range(0,4):\n",
        "    plt.subplot(2,2,res+1)\n",
        "    plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(history_mdl_tpl_IV[res].history['loss'], label=\"MLP - Treino\")\n",
        "    plt.plot(history_mdl_tpl_IV[res].history['val_loss'], label=\"MLP - Validação\")\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y2GSR4IdE-VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Avaliação**"
      ],
      "metadata": {
        "id": "RPLnepOrE-VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA IV: AVALIAÇÃO', size=14)\n",
        "\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = mdl_tpl_IV[res].predict(x_train)\n",
        "  plt.plot(x_train, y_train, '.', color='#4682B4')\n",
        "  plt.plot(x_train, y_rest, '.',color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M7VR8TKfE-VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA IV: AVALIAÇÃO', size=14)\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = mdl_tpl_IV[res].predict(x_test)\n",
        "  plt.plot(x_test, y_test, '.', color='#4682B4')\n",
        "  plt.plot(x_test, y_rest, '.', markersize=3 ,color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for res in range(0,4):\n",
        "  test_loss = mdl_tpl_IV[res].evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "h__QL27KE-VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_mdl_tpl_IV = []\n",
        "for res in range(0,4): \n",
        "  score_mdl_tpl_IV.append(mdl_tpl_IV[res].evaluate(x_test, y_test, verbose=0))\n",
        "  print('Topologia III - Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  print('Teste loss:',   score_mdl_tpl_IV[res][0])\n",
        "  print('Teste métricas:', score_mdl_tpl_IV[res][1])\n",
        "  print('----------------------------------------')"
      ],
      "metadata": {
        "id": "R6oaL8u5E-VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topologia V**\n",
        "\n",
        "A rede neural que será criada será uma MLP com 6 camadas, sendo:\n",
        "*   01 camada de entrada (input layer) com 50 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 50 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 50 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 40 Neurônios\n",
        "*   01 camada escondida (hidden layer) com 20 Neurônios\n",
        "*   01 camada de saída (output layer) com 1 Neurônio"
      ],
      "metadata": {
        "id": "GQJY3zPss3rU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criação do Modelos**"
      ],
      "metadata": {
        "id": "SCXA8O1qs3rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelos da Topologia V\n",
        "mdl_tpl_V = []\n",
        "\n",
        "for res in range(0,4):\n",
        "  #print(i)\n",
        "  sgd = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=momentum[res])\n",
        "  mdl_tpl_V.append(keras.Sequential())\n",
        "  mdl_tpl_V[res].add(keras.layers.Dense(units=50,  kernel_regularizer=l2(regularizer[res]), input_shape=[1], activation=\"tanh\"))\n",
        "  mdl_tpl_V[res].add(Dropout(dropout[res]))\n",
        "  mdl_tpl_V[res].add(keras.layers.Dense(units=50,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_V[res].add(Dropout(dropout[res]))\n",
        "  mdl_tpl_V[res].add(keras.layers.Dense(units=50,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_V[res].add(Dropout(dropout[res]))\n",
        "  mdl_tpl_V[res].add(keras.layers.Dense(units=40,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_V[res].add(Dropout(dropout[res]))\n",
        "  mdl_tpl_V[res].add(keras.layers.Dense(units=20,  kernel_regularizer=l2(regularizer[res]), activation=\"tanh\"))\n",
        "  mdl_tpl_V[res].add(keras.layers.Dense(units=1, activation=\"linear\"))\n",
        "  #tf.keras.utils.plot_model(mdl_tpl_V[res], show_shapes=True)\n",
        "  mdl_tpl_V[res].compile(loss='mean_squared_error', optimizer=sgd, metrics=[\"mean_absolute_error\"])"
      ],
      "metadata": {
        "id": "d_SUAuGVs3rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agora vamos treinar os Modelos criados**"
      ],
      "metadata": {
        "id": "dfk1iU1is3rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
        "\n",
        "history_mdl_tpl_V = []\n",
        "for res in range(0,4):\n",
        "  history_mdl_tpl_V.append(mdl_tpl_V[res].fit(x_train, y_train, epochs=epochs, batch_size=10, validation_split=0.2, verbose=True)) #callbacks=[early]\n"
      ],
      "metadata": {
        "id": "dHodAv1As3rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados do Treinamento**"
      ],
      "metadata": {
        "id": "fwsgmROgs3rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA V: EVOLUÇÃO DOS TREINAMENTOS', size=14)\n",
        "for res in range(0,4):\n",
        "    plt.subplot(2,2,res+1)\n",
        "    plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(history_mdl_tpl_V[res].history['loss'], label=\"MLP - Treino\")\n",
        "    plt.plot(history_mdl_tpl_V[res].history['val_loss'], label=\"MLP - Validação\")\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BLTrX4x4s3rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Avaliação**"
      ],
      "metadata": {
        "id": "Eu2nN3qes3rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA V: AVALIAÇÃO', size=14)\n",
        "\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = mdl_tpl_V[res].predict(x_train)\n",
        "  plt.plot(x_train, y_train, '.', color='#4682B4')\n",
        "  plt.plot(x_train, y_rest, '.',color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N8K0CBXas3rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(16,9),dpi=80)\n",
        "plt.suptitle('TOPOLOGIA V: AVALIAÇÃO', size=14)\n",
        "for res in range(0,4):\n",
        "\n",
        "  plt.subplot(2,2,res+1)\n",
        "  y_rest = mdl_tpl_V[res].predict(x_test)\n",
        "  plt.plot(x_test, y_test, '.', color='#4682B4')\n",
        "  plt.plot(x_test, y_rest, '.', markersize=3 ,color='#FF4500')\n",
        "  plt.title('Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for res in range(0,4):\n",
        "  test_loss = mdl_tpl_V[res].evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "r2i0-F-vs3rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_mdl_tpl_V = []\n",
        "for res in range(0,4): \n",
        "  score_mdl_tpl_V.append(mdl_tpl_V[res].evaluate(x_test, y_test, verbose=0))\n",
        "  print('Topologia III - Modelo '+ str(res+1) + ': Optimizer(SGD), Regularizer L2('+str(regularizer[res])+'), Momentum('+str(momentum[res])+')')\n",
        "  print('Teste loss:',   score_mdl_tpl_V[res][0])\n",
        "  print('Teste métricas:', score_mdl_tpl_V[res][1])\n",
        "  print('----------------------------------------')"
      ],
      "metadata": {
        "id": "NpGsdJhWs3rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problema 2: Classificação**"
      ],
      "metadata": {
        "id": "4rCuKURC35Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos coletar apenas uma parte do Dataset para usá-lo neste Projeto\n",
        "'''split_date = datetime.datetime(year=2022, month=6,day=1)\n",
        "\n",
        "df_train_rd80 = df_rd80.loc[df_rd80.index < str(split_date)]\n",
        "df_val_rd80 = df_rd80.loc[df_rd80.index >= str(split_date)]\n",
        "\n",
        "df_train_rd80.reset_index(inplace=True)\n",
        "df_val_rd80.reset_index(inplace=True)\n",
        "\n",
        "print('Shape do treinamento: ', df_train_rd80.shape)\n",
        "print('Shape da validação: ', df_val_rd80.shape)\n",
        "df_train_rd80.head(n=20)'''"
      ],
      "metadata": {
        "id": "_9FTcR2PTiaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizando as correlações entre as variáveis\n",
        "corr = df_rd80.corr()\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(corr[(corr >= 0.30) | (corr <= -0.30)],cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1, annot=True, annot_kws={\"size\": 14}, square=True);"
      ],
      "metadata": {
        "id": "2Lmy3vD-8Is-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Considerações Finais**\n",
        "\n",
        "As redes MLP é uma boa opção para ser aplicada em problemas de regressão. Com base nos históricos obtidos nos treinamentos e testes em cada uma das topologias, aplicando a variação nos hyperparâmetros, número de camadas e números de neurônios, foi  possível visualizar qual a melhor configuração e melhor modelo para ser apicado ao problema em questão."
      ],
      "metadata": {
        "id": "02azJBf2w61y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Referências**\n",
        "\n",
        "BASHEER, I. A.; HAJMEER, M. Artificial neural networks: fundamentals, computing, design, and application. Journal of Microbiological Methods, v. 43, p. 3–31, 2000\n",
        "\n",
        "HAYKIN, Simon. Redes Neurais : princípios e prática. Traduzido por Paulo Martins Engel. 2. ed. Porto Alegre : Bookman, 2001. 900 p. il.\n",
        "\n",
        "HSU, K. L.; GAO, X.; SOROOSHIAN, S.; GUPTA, H. V. Precipitation estimation from remotely sensed information using artificial neural networks. Journal of Applied Meteorology, v. 36, n. 9, p. 1176-1190, 1997.\n",
        "\n",
        "https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8\n",
        "\n",
        "https://sites.icmc.usp.br/andre/research/neural/MLP.htm\n",
        "\n",
        "KOVAKS, Z. L. Redes Neurais Artificiais: fundamentos e Aplicações. São Paulo: Acadêmica, 1996.\n",
        "\n",
        "TAFNER, Malcon A.; XEREZ, Marcos de; RODRIGUES FILHO, Ilson W. Redes neurais artificiais : introdução e princípios de neurocomputação. Blumenau : Eko, 1995."
      ],
      "metadata": {
        "id": "rTspcgyB5i7Z"
      }
    }
  ]
}